{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition\n",
    "1. Multiple online kaggle tutorials were referred while working on this lab.\n",
    "2. Some contents are heavily inspired by the methods and codes Kagglers have used in their solutions.\n",
    "3. A reference list is provide at the end of pdf.\n",
    "\n",
    "### Here I am only performing an exhaustive gridsearch for several classifiers so that I can narrow down on the params\n",
    "\n",
    "I use GridSearchCV with the same k-Fold values and the Random Seed. Fix it to 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Fold Predictions for Ensembles\n",
    "Another common use for out-of-fold predictions is to use them in the development of an ensemble model.\n",
    "\n",
    "An ensemble is a machine learning model that combines the predictions from two or more models prepared on the same training dataset.\n",
    "\n",
    "This is a very common procedure to use when working on a machine learning competition.\n",
    "\n",
    "The out-of-fold predictions in aggregate provide information about how the model performs on each example in the training dataset when not used to train the model. This information can be used to train a model to correct or improve upon those predictions.\n",
    "\n",
    "First, the k-fold cross-validation procedure is performed on each base model of interest, and all of the out-of-fold predictions are collected. Importantly, the same split of the training data into k-folds is performed for each model. Now we have one aggregated group of out-of-sample predictions for each model, e.g. predictions for each example in the training dataset.\n",
    "\n",
    "* Base-Models: Models evaluated using k-fold cross-validation on the training dataset and all out-of-fold predictions are retained.\n",
    "Next, a second higher-order model, called a meta-model, is trained on the predictions made by the other models. This meta-model may or may not also take the input data for each example as input when making predictions. The job of this model is to learn how to best combine and correct the predictions made by the other models using their out-of-fold predictions.\n",
    "\n",
    "* Meta-Model: Model that takes the out-of-fold predictions made by one or more models as input and shows how to best combine and correct the predictions.\n",
    "For example, we may have a two-class classification predictive modeling problem and train a decision tree and a k-nearest neighbor model as the base models. Each model predicts a 0 or 1 for each example in the training dataset via out-of-fold predictions. These predictions, along with the input data, can then form a new input to the meta-model.\n",
    "\n",
    "* Meta-Model Input: Input portion of a given sample concatenated with the predictions made by each base model.\n",
    "* Meta-Model Output: Output portion of a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n",
    "%matplotlib inline\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/local/lambda/rishabhs/ML/data_science_lab/kagglecomp'\n",
    "train = pd.read_csv(\"data/data-science-comp-f2020/train_final.csv\")\n",
    "test = pd.read_csv(\"data/data-science-comp-f2020/test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25884</td>\n",
       "      <td>1</td>\n",
       "      <td>33.63</td>\n",
       "      <td>118596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118595</td>\n",
       "      <td>125738</td>\n",
       "      <td>...</td>\n",
       "      <td>1945</td>\n",
       "      <td>118450</td>\n",
       "      <td>119184</td>\n",
       "      <td>1</td>\n",
       "      <td>121372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34346</td>\n",
       "      <td>1</td>\n",
       "      <td>10.62</td>\n",
       "      <td>118041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117902</td>\n",
       "      <td>130913</td>\n",
       "      <td>...</td>\n",
       "      <td>15385</td>\n",
       "      <td>117945</td>\n",
       "      <td>292795</td>\n",
       "      <td>1</td>\n",
       "      <td>259173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34923</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>118327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>124402</td>\n",
       "      <td>...</td>\n",
       "      <td>7547</td>\n",
       "      <td>118933</td>\n",
       "      <td>290919</td>\n",
       "      <td>1</td>\n",
       "      <td>118784</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>80926</td>\n",
       "      <td>1</td>\n",
       "      <td>30.09</td>\n",
       "      <td>118300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>301218</td>\n",
       "      <td>...</td>\n",
       "      <td>4933</td>\n",
       "      <td>118458</td>\n",
       "      <td>118331</td>\n",
       "      <td>1</td>\n",
       "      <td>307024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4674</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>119921</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119920</td>\n",
       "      <td>302830</td>\n",
       "      <td>...</td>\n",
       "      <td>13836</td>\n",
       "      <td>142145</td>\n",
       "      <td>4673</td>\n",
       "      <td>1</td>\n",
       "      <td>128230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Y     f1  f2     f3      f4  f5  f6      f7      f8  ...    f15  \\\n",
       "0   1  1  25884   1  33.63  118596   1   0  118595  125738  ...   1945   \n",
       "1   2  1  34346   1  10.62  118041   1   0  117902  130913  ...  15385   \n",
       "2   3  1  34923   1   1.77  118327   1   0  117961  124402  ...   7547   \n",
       "3   4  1  80926   1  30.09  118300   1   0  117961  301218  ...   4933   \n",
       "4   5  1   4674   1   1.77  119921   1   0  119920  302830  ...  13836   \n",
       "\n",
       "      f16     f17  f18     f19  f20  f21  f22  f23  f24  \n",
       "0  118450  119184    1  121372    1    1    1    2    1  \n",
       "1  117945  292795    1  259173    1    1    1    1    1  \n",
       "2  118933  290919    1  118784    1    1    1    1    1  \n",
       "3  118458  118331    1  307024    1    1    1    2    1  \n",
       "4  142145    4673    1  128230    1    1    1  620    1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16383, 26)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['Id'],axis=1)\n",
    "test = test.drop(['Id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25884</td>\n",
       "      <td>1</td>\n",
       "      <td>33.63</td>\n",
       "      <td>118596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118595</td>\n",
       "      <td>125738</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1945</td>\n",
       "      <td>118450</td>\n",
       "      <td>119184</td>\n",
       "      <td>1</td>\n",
       "      <td>121372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34346</td>\n",
       "      <td>1</td>\n",
       "      <td>10.62</td>\n",
       "      <td>118041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117902</td>\n",
       "      <td>130913</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>15385</td>\n",
       "      <td>117945</td>\n",
       "      <td>292795</td>\n",
       "      <td>1</td>\n",
       "      <td>259173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>34923</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>118327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>124402</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7547</td>\n",
       "      <td>118933</td>\n",
       "      <td>290919</td>\n",
       "      <td>1</td>\n",
       "      <td>118784</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>80926</td>\n",
       "      <td>1</td>\n",
       "      <td>30.09</td>\n",
       "      <td>118300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>301218</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4933</td>\n",
       "      <td>118458</td>\n",
       "      <td>118331</td>\n",
       "      <td>1</td>\n",
       "      <td>307024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4674</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>119921</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119920</td>\n",
       "      <td>302830</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13836</td>\n",
       "      <td>142145</td>\n",
       "      <td>4673</td>\n",
       "      <td>1</td>\n",
       "      <td>128230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y     f1  f2     f3      f4  f5  f6      f7      f8  f9  ...    f15  \\\n",
       "0  1  25884   1  33.63  118596   1   0  118595  125738   1  ...   1945   \n",
       "1  1  34346   1  10.62  118041   1   0  117902  130913   1  ...  15385   \n",
       "2  1  34923   1   1.77  118327   1   0  117961  124402   1  ...   7547   \n",
       "3  1  80926   1  30.09  118300   1   0  117961  301218   1  ...   4933   \n",
       "4  1   4674   1   1.77  119921   1   0  119920  302830   1  ...  13836   \n",
       "\n",
       "      f16     f17  f18     f19  f20  f21  f22  f23  f24  \n",
       "0  118450  119184    1  121372    1    1    1    2    1  \n",
       "1  117945  292795    1  259173    1    1    1    1    1  \n",
       "2  118933  290919    1  118784    1    1    1    1    1  \n",
       "3  118458  118331    1  307024    1    1    1    2    1  \n",
       "4  142145    4673    1  128230    1    1    1  620    1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "target = \"Y\"\n",
    "col4train = [x for x in train.columns if x not in [target, \"f13\"]]\n",
    "y = train[target].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25884</td>\n",
       "      <td>1</td>\n",
       "      <td>33.63</td>\n",
       "      <td>118596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118595</td>\n",
       "      <td>125738</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1945</td>\n",
       "      <td>118450</td>\n",
       "      <td>119184</td>\n",
       "      <td>1</td>\n",
       "      <td>121372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34346</td>\n",
       "      <td>1</td>\n",
       "      <td>10.62</td>\n",
       "      <td>118041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117902</td>\n",
       "      <td>130913</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>15385</td>\n",
       "      <td>117945</td>\n",
       "      <td>292795</td>\n",
       "      <td>1</td>\n",
       "      <td>259173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>34923</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>118327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>124402</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7547</td>\n",
       "      <td>118933</td>\n",
       "      <td>290919</td>\n",
       "      <td>1</td>\n",
       "      <td>118784</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>80926</td>\n",
       "      <td>1</td>\n",
       "      <td>30.09</td>\n",
       "      <td>118300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>301218</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4933</td>\n",
       "      <td>118458</td>\n",
       "      <td>118331</td>\n",
       "      <td>1</td>\n",
       "      <td>307024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4674</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>119921</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119920</td>\n",
       "      <td>302830</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13836</td>\n",
       "      <td>142145</td>\n",
       "      <td>4673</td>\n",
       "      <td>1</td>\n",
       "      <td>128230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y     f1  f2     f3      f4  f5  f6      f7      f8  f9  ...    f15  \\\n",
       "0  1  25884   1  33.63  118596   1   0  118595  125738   1  ...   1945   \n",
       "1  1  34346   1  10.62  118041   1   0  117902  130913   1  ...  15385   \n",
       "2  1  34923   1   1.77  118327   1   0  117961  124402   1  ...   7547   \n",
       "3  1  80926   1  30.09  118300   1   0  117961  301218   1  ...   4933   \n",
       "4  1   4674   1   1.77  119921   1   0  119920  302830   1  ...  13836   \n",
       "\n",
       "      f16     f17  f18     f19  f20  f21  f22  f23  f24  \n",
       "0  118450  119184    1  121372    1    1    1    2    1  \n",
       "1  117945  292795    1  259173    1    1    1    1    1  \n",
       "2  118933  290919    1  118784    1    1    1    1    1  \n",
       "3  118458  118331    1  307024    1    1    1    2    1  \n",
       "4  142145    4673    1  128230    1    1    1  620    1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a252c6ad7769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load all the saved models to validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the model from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#xgbBest = joblib.load('xgbBest.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "## Load all the saved models to validate\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "# Load the model from the file \n",
    "#xgbBest = joblib.load('xgbBest.pkl')  \n",
    "\n",
    "#model4_catBoost = joblib.load('model4_catBoost.pkl')  \n",
    "\n",
    "#stack_gen_model2 = joblib.load('stack_gen_model2.pkl')\n",
    "\n",
    "CatBoost_best_encdata = joblib.load('CatBoost_best_encdata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TargetEncoding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_names ):\n",
    "        self.columns_names = columns_names\n",
    "        self.learned_values = {}\n",
    "        self.dataset_mean = np.nan\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        X_ = X.copy()\n",
    "        self.learned_values = {}\n",
    "        X_[\"__target__\"] = y\n",
    "        for c in [x for x in X_.columns if x in self.columns_names]:\n",
    "            self.learned_values[c] = (X_[[c,\"__target__\"]]\n",
    "                                      .groupby(c)[\"__target__\"].mean()\n",
    "                                      .reset_index())\n",
    "        self.dataset_mean = np.mean(y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **fit_params):\n",
    "        transformed_X = X[self.columns_names].copy()\n",
    "        for c in transformed_X.columns:\n",
    "            transformed_X[c] = (transformed_X[[c]]\n",
    "                                .merge(self.learned_values[c], on = c, how = 'left')\n",
    "                               )[\"__target__\"]\n",
    "        transformed_X = transformed_X.fillna(self.dataset_mean)\n",
    "        return transformed_X\n",
    "    \n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TargetEncodingSmoothing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_names,k, f ):\n",
    "        self.columns_names = columns_names\n",
    "        self.learned_values = {}\n",
    "        self.dataset_mean = np.nan\n",
    "        self.k = k #\n",
    "        self.f = f #\n",
    "    def smoothing_func(self, N): #\n",
    "        return 1 / (1 + np.exp(-(N-self.k)/self.f))\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        X_ = X.copy()\n",
    "        self.learned_values = {}\n",
    "        self.dataset_mean = np.mean(y)\n",
    "        X_[\"__target__\"] = y\n",
    "        for c in [x for x in X_.columns if x in self.columns_names]:\n",
    "            stats = (X_[[c,\"__target__\"]]\n",
    "                     .groupby(c)[\"__target__\"].\n",
    "                     agg(['mean', 'size'])) \n",
    "            stats[\"alpha\"] = self.smoothing_func(stats[\"size\"])\n",
    "            stats[\"__target__\"] = (stats[\"alpha\"]*stats[\"mean\"] \n",
    "                                   + (1-stats[\"alpha\"])*self.dataset_mean)\n",
    "            stats = (stats\n",
    "                     .drop([x for x in stats.columns if x not in [\"__target__\",c]], axis = 1)\n",
    "                     .reset_index())\n",
    "            self.learned_values[c] = stats\n",
    "        self.dataset_mean = np.mean(y)\n",
    "        return self\n",
    "    def transform(self, X, **fit_params):\n",
    "        transformed_X = X[self.columns_names].copy()\n",
    "        for c in transformed_X.columns:\n",
    "            transformed_X[c] = (transformed_X[[c]]\n",
    "                                .merge(self.learned_values[c], on = c, how = 'left')\n",
    "                               )[\"__target__\"]\n",
    "        transformed_X = transformed_X.fillna(self.dataset_mean)\n",
    "        return transformed_X\n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noise. CV inside CV.Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CV_target_encoding(data, y, encoder, cv = 5):\n",
    "    skfTE = StratifiedKFold(n_splits=cv, random_state = 545167, shuffle = True)\n",
    "    result = []\n",
    "    for train_indexTE, test_indexTE in skfTE.split(data, y):\n",
    "        encoder.fit(data.iloc[train_indexTE,:].reset_index(drop = True), y[train_indexTE])\n",
    "        tmp =  encoder.transform(data.iloc[test_indexTE,:].reset_index(drop = True))\n",
    "        tmp[\"index\"] = test_indexTE\n",
    "        result.append(tmp)\n",
    "    result = pd.concat(result, ignore_index = True)\n",
    "    result = result.sort_values('index').reset_index(drop = True).drop('index', axis = 1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noise. Expanding mean\n",
    "\n",
    "Next idea how to add noise is called expanding mean and you will now understand why.\n",
    "\n",
    "Imagine algorithm rolling trough data and for each new row it uses all previously seen rows to calculate this new row mean. For the very first row there is no previously seen rows available so it's mean will be dataset mean. For the second row you can use first (and only first) row, because you already saw it.\n",
    "\n",
    "This approach especially suited for streaming (that is if you have infinite stream of data coming to you).\n",
    "\n",
    "### Advantages\n",
    "\n",
    "Powerful, task-specific encoding\n",
    "### Disadvantages\n",
    "\n",
    "Can introduce too much noise :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncodingExpandingMean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_names):\n",
    "        self.columns_names = columns_names\n",
    "        self.learned_values = {}\n",
    "        self.dataset_mean = np.nan\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        X_ = X.copy()\n",
    "        self.learned_values = {}\n",
    "        self.dataset_mean = np.mean(y)\n",
    "        X_[\"__target__\"] = y\n",
    "        for c in [x for x in X_.columns if x in self.columns_names]:\n",
    "            stats = (X_[[c,\"__target__\"]]\n",
    "                     .groupby(c)[\"__target__\"]\n",
    "                     .agg(['mean', 'size'])) #\n",
    "            stats[\"__target__\"] = stats[\"mean\"]\n",
    "            stats = (stats\n",
    "                     .drop([x for x in stats.columns if x not in [\"__target__\",c]], axis = 1)\n",
    "                     .reset_index())\n",
    "            self.learned_values[c] = stats\n",
    "        return self\n",
    "    def transform(self, X, **fit_params):\n",
    "        transformed_X = X[self.columns_names].copy()\n",
    "        for c in transformed_X.columns:\n",
    "            transformed_X[c] = (transformed_X[[c]]\n",
    "                                .merge(self.learned_values[c], on = c, how = 'left')\n",
    "                               )[\"__target__\"]\n",
    "        transformed_X = transformed_X.fillna(self.dataset_mean)\n",
    "        return transformed_X\n",
    "    \n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        self.fit(X,y)\n",
    "    \n",
    "        #Expanding mean transform\n",
    "        X_ = X[self.columns_names].copy().reset_index(drop = True)\n",
    "        X_[\"__target__\"] = y\n",
    "        X_[\"index\"] = X_.index\n",
    "        X_transformed = pd.DataFrame()\n",
    "        for c in self.columns_names:\n",
    "            X_shuffled = X_[[c,\"__target__\", \"index\"]].copy()\n",
    "            X_shuffled = X_shuffled.sample(n = len(X_shuffled),replace=False)\n",
    "            X_shuffled[\"cnt\"] = 1\n",
    "            X_shuffled[\"cumsum\"] = (X_shuffled\n",
    "                                    .groupby(c,sort=False)['__target__']\n",
    "                                    .apply(lambda x : x.shift().cumsum()))\n",
    "            X_shuffled[\"cumcnt\"] = (X_shuffled\n",
    "                                    .groupby(c,sort=False)['cnt']\n",
    "                                    .apply(lambda x : x.shift().cumsum()))\n",
    "            X_shuffled[\"encoded\"] = X_shuffled[\"cumsum\"] / X_shuffled[\"cumcnt\"]\n",
    "            X_shuffled[\"encoded\"] = X_shuffled[\"encoded\"].fillna(self.dataset_mean)\n",
    "            X_transformed[c] = X_shuffled.sort_values(\"index\")[\"encoded\"].values\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why don't we add some new features? How? Let's use feature pairs to create a new set of categorical features. Just take pair of existing features and concat them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[col4train] = train[col4train].values.astype(str)\n",
    "test[col4train] = test[col4train].values.astype(str)\n",
    "\n",
    "from itertools import combinations\n",
    "new_col4train = col4train\n",
    "for c1,c2 in combinations(col4train, 2):\n",
    "    name = \"{}_{}\".format(c1,c2)\n",
    "    new_col4train.append(name)\n",
    "    train[name] = train[c1] + \"_\" + train[c2]\n",
    "    test[name] = test[c1] + \"_\" + test[c2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16383, 276) (16385, 276)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f20_f21</th>\n",
       "      <th>f20_f22</th>\n",
       "      <th>f20_f23</th>\n",
       "      <th>f20_f24</th>\n",
       "      <th>f21_f22</th>\n",
       "      <th>f21_f23</th>\n",
       "      <th>f21_f24</th>\n",
       "      <th>f22_f23</th>\n",
       "      <th>f22_f24</th>\n",
       "      <th>f23_f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25884.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.63</td>\n",
       "      <td>118596.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118595.0</td>\n",
       "      <td>125738.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>2.0_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34346.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.62</td>\n",
       "      <td>118041.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117902.0</td>\n",
       "      <td>130913.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34923.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>118327.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117961.0</td>\n",
       "      <td>124402.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80926.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.09</td>\n",
       "      <td>118300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117961.0</td>\n",
       "      <td>301218.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_2.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>2.0_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4674.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>119921.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119920.0</td>\n",
       "      <td>302830.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_620.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_620.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>1.0_620.0</td>\n",
       "      <td>1.0_1.0</td>\n",
       "      <td>620.0_1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1   f2     f3        f4   f5   f6        f7        f8   f9  f10  ...  \\\n",
       "0  25884.0  1.0  33.63  118596.0  1.0  0.0  118595.0  125738.0  1.0  3.0  ...   \n",
       "1  34346.0  1.0  10.62  118041.0  1.0  0.0  117902.0  130913.0  1.0  1.0  ...   \n",
       "2  34923.0  1.0   1.77  118327.0  1.0  0.0  117961.0  124402.0  1.0  2.0  ...   \n",
       "3  80926.0  1.0  30.09  118300.0  1.0  0.0  117961.0  301218.0  1.0  0.0  ...   \n",
       "4   4674.0  1.0   1.77  119921.0  1.0  0.0  119920.0  302830.0  1.0  0.0  ...   \n",
       "\n",
       "   f20_f21  f20_f22    f20_f23  f20_f24  f21_f22    f21_f23  f21_f24  \\\n",
       "0  1.0_1.0  1.0_1.0    1.0_2.0  1.0_1.0  1.0_1.0    1.0_2.0  1.0_1.0   \n",
       "1  1.0_1.0  1.0_1.0    1.0_1.0  1.0_1.0  1.0_1.0    1.0_1.0  1.0_1.0   \n",
       "2  1.0_1.0  1.0_1.0    1.0_1.0  1.0_1.0  1.0_1.0    1.0_1.0  1.0_1.0   \n",
       "3  1.0_1.0  1.0_1.0    1.0_2.0  1.0_1.0  1.0_1.0    1.0_2.0  1.0_1.0   \n",
       "4  1.0_1.0  1.0_1.0  1.0_620.0  1.0_1.0  1.0_1.0  1.0_620.0  1.0_1.0   \n",
       "\n",
       "     f22_f23  f22_f24    f23_f24  \n",
       "0    1.0_2.0  1.0_1.0    2.0_1.0  \n",
       "1    1.0_1.0  1.0_1.0    1.0_1.0  \n",
       "2    1.0_1.0  1.0_1.0    1.0_1.0  \n",
       "3    1.0_2.0  1.0_1.0    2.0_1.0  \n",
       "4  1.0_620.0  1.0_1.0  620.0_1.0  \n",
       "\n",
       "[5 rows x 276 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train[new_col4train].shape, test[new_col4train].shape)\n",
    "train[new_col4train].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1         5170\n",
       "f2            7\n",
       "f3          168\n",
       "f4          162\n",
       "f5            8\n",
       "           ... \n",
       "f21_f23    1012\n",
       "f21_f24      19\n",
       "f22_f23    1003\n",
       "f22_f24      19\n",
       "f23_f24    1008\n",
       "Length: 276, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[new_col4train].apply(lambda x: len(x.unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a lot of them are high-cardinality categorical features. Luckily for us we now know how to handle them. Let's use both TargetEncodingExpandingMean and TargetEncodingSmoothing with CV to create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Baseline model trained with X_train and y_train subset; eval is the X_val and y_val subset\n",
    "# model = CatBoostClassifier(custom_metric=['TotalF1'], early_stopping_rounds=200, eval_metric='AUC', n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f15', 'f16', \\\n",
    "              'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f1_f2', 'f1_f3', 'f1_f4', 'f1_f5', 'f1_f6', \\\n",
    "              'f1_f7', 'f1_f8', 'f1_f9', 'f1_f10', 'f1_f11', 'f1_f12', 'f1_f14', 'f1_f15', 'f1_f16', 'f1_f17', \\\n",
    "              'f1_f18', 'f1_f19', 'f1_f20', 'f1_f21', 'f1_f22', 'f1_f23', 'f1_f24', 'f2_f3', 'f2_f4', 'f2_f5', \\\n",
    "              'f2_f6', 'f2_f7', 'f2_f8', 'f2_f9', 'f2_f10', 'f2_f11', 'f2_f12', 'f2_f14', 'f2_f15', 'f2_f16', \\\n",
    "              'f2_f17', 'f2_f18', 'f2_f19', 'f2_f20', 'f2_f21', 'f2_f22', 'f2_f23', 'f2_f24', 'f3_f4', 'f3_f5', \\\n",
    "              'f3_f6', 'f3_f7', 'f3_f8', 'f3_f9', 'f3_f10', 'f3_f11', 'f3_f12', 'f3_f14', 'f3_f15', 'f3_f16', \\\n",
    "              'f3_f17', 'f3_f18', 'f3_f19', 'f3_f20', 'f3_f21', 'f3_f22', 'f3_f23', 'f3_f24', 'f4_f5', 'f4_f6', \\\n",
    "              'f4_f7', 'f4_f8', 'f4_f9', 'f4_f10', 'f4_f11', 'f4_f12', 'f4_f14', 'f4_f15', 'f4_f16', 'f4_f17', \\\n",
    "              'f4_f18', 'f4_f19', 'f4_f20', 'f4_f21', 'f4_f22', 'f4_f23', 'f4_f24', 'f5_f6', 'f5_f7', 'f5_f8', \\\n",
    "              'f5_f9', 'f5_f10', 'f5_f11', 'f5_f12', 'f5_f14', 'f5_f15', 'f5_f16', 'f5_f17', 'f5_f18', 'f5_f19', \\\n",
    "              'f5_f20', 'f5_f21', 'f5_f22', 'f5_f23', 'f5_f24', 'f6_f7', 'f6_f8', 'f6_f9', 'f6_f10', 'f6_f11', \\\n",
    "              'f6_f12', 'f6_f14', 'f6_f15', 'f6_f16', 'f6_f17', 'f6_f18', 'f6_f19', 'f6_f20', 'f6_f21', 'f6_f22', \\\n",
    "              'f6_f23', 'f6_f24', 'f7_f8', 'f7_f9', 'f7_f10', 'f7_f11', 'f7_f12', 'f7_f14', 'f7_f15', 'f7_f16', \\\n",
    "              'f7_f17', 'f7_f18', 'f7_f19', 'f7_f20', 'f7_f21', 'f7_f22', 'f7_f23', 'f7_f24', 'f8_f9', 'f8_f10', \\\n",
    "              'f8_f11', 'f8_f12', 'f8_f14', 'f8_f15', 'f8_f16', 'f8_f17', 'f8_f18', 'f8_f19', 'f8_f20', 'f8_f21', \\\n",
    "              'f8_f22', 'f8_f23', 'f8_f24', 'f9_f10', 'f9_f11', 'f9_f12', 'f9_f14', 'f9_f15', 'f9_f16', 'f9_f17', \\\n",
    "              'f9_f18', 'f9_f19', 'f9_f20', 'f9_f21', 'f9_f22', 'f9_f23', 'f9_f24', 'f10_f11', 'f10_f12', 'f10_f14', \\\n",
    "              'f10_f15', 'f10_f16', 'f10_f17', 'f10_f18', 'f10_f19', 'f10_f20', 'f10_f21', 'f10_f22', 'f10_f23', 'f10_f24', \\\n",
    "              'f11_f12', 'f11_f14', 'f11_f15', 'f11_f16', 'f11_f17', 'f11_f18', 'f11_f19', 'f11_f20', 'f11_f21', 'f11_f22', \\\n",
    "              'f11_f23', 'f11_f24', 'f12_f14', 'f12_f15', 'f12_f16', 'f12_f17', 'f12_f18', 'f12_f19', 'f12_f20', 'f12_f21', \\\n",
    "              'f12_f22', 'f12_f23', 'f12_f24', 'f14_f15', 'f14_f16', 'f14_f17', 'f14_f18', 'f14_f19', 'f14_f20', 'f14_f21', \\\n",
    "              'f14_f22', 'f14_f23', 'f14_f24', 'f15_f16', 'f15_f17', 'f15_f18', 'f15_f19', 'f15_f20', 'f15_f21', 'f15_f22', \\\n",
    "              'f15_f23', 'f15_f24', 'f16_f17', 'f16_f18', 'f16_f19', 'f16_f20', 'f16_f21', 'f16_f22', 'f16_f23', 'f16_f24', \\\n",
    "              'f17_f18', 'f17_f19', 'f17_f20', 'f17_f21', 'f17_f22', 'f17_f23', 'f17_f24', 'f18_f19', 'f18_f20', 'f18_f21', \\\n",
    "              'f18_f22', 'f18_f23', 'f18_f24', 'f19_f20', 'f19_f21', 'f19_f22', 'f19_f23', 'f19_f24', 'f20_f21', 'f20_f22', \\\n",
    "              'f20_f23', 'f20_f24', 'f21_f22', 'f21_f23', 'f21_f24', 'f22_f23', 'f22_f24', 'f23_f24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "model_xgBoost = XGBClassifier(\n",
    "                       n_estimators=10000,\n",
    "                       booster='gbtree',\n",
    "                       max_depth=5,\n",
    "                       gamma=0.5,\n",
    "                       subsample=0.8,\n",
    "                       colsample_bytree=0.6,\n",
    "                       objective='binary:logistic',\n",
    "                       nthread=1,\n",
    "                       n_jobs=-1,\n",
    "                       min_child_weight=1,\n",
    "                       leaning_rate=0.05,\n",
    "                       silent=0,\n",
    "                       seed=42,\n",
    "                       reg_alpha=0.0006,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       #predictor = 'gpu_predictor',\n",
    "                       #gpu_id=0,\n",
    "                       random_state=1001)\n",
    "\n",
    "model = model_xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniquify(df_columns):\n",
    "    seen = set()\n",
    "\n",
    "    for item in df_columns:\n",
    "        fudge = 1\n",
    "        newitem = item\n",
    "\n",
    "        while newitem in seen:\n",
    "            fudge += 1\n",
    "            newitem = \"{}_{}\".format(item, fudge)\n",
    "\n",
    "        yield newitem\n",
    "        seen.add(newitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import time\n",
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(sample_submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 24 artists>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare number of Unique Categorical labels for train and test\n",
    "\n",
    "unique_train= pd.DataFrame([(col,train[col].nunique()) for col in train.columns], \n",
    "                           columns=['Columns', 'Unique categories'])\n",
    "unique_test=pd.DataFrame([(col,test[col].nunique()) for col in test.columns],\n",
    "                columns=['Columns', 'Unique categories'])\n",
    "unique_train=unique_train[1:]\n",
    "unique_test=unique_test[1:]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ax[0].bar(unique_train.Columns, unique_train['Unique categories'])\n",
    "ax[1].bar(unique_test.Columns, unique_test['Unique categories'])\n",
    "#plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Y', ylabel='count'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicated rows\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows\n",
    "\n",
    "if (sum(train.duplicated()), sum(test.duplicated())) == (0,0):\n",
    "    print('No duplicated rows')\n",
    "else: \n",
    "    print('train: ',sum(train.duplicated()))\n",
    "    print('test: ',sum(train.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f13 f19\n",
      "Potential Categorical column duplication\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated columns                          \n",
    "from itertools import combinations\n",
    "\n",
    "for col1,col2 in combinations(train.columns, 2):\n",
    "    condition1=len(train.groupby([col1,col2]).size())==len(train.groupby([col1]).size())\n",
    "    condition2=len(train.groupby([col1,col2]).size())==len(train.groupby([col2]).size())\n",
    "    condition3=(train[col1].nunique()==train[col2].nunique())\n",
    "    if (condition1 | condition2) & condition3:\n",
    "        print(col1,col2)\n",
    "        print('Potential Categorical column duplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f13</th>\n",
       "      <th>f19</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117880</th>\n",
       "      <th>117879</th>\n",
       "      <td>8201.386792</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>37070.622642</td>\n",
       "      <td>1.042453</td>\n",
       "      <td>8.744245</td>\n",
       "      <td>116838.770440</td>\n",
       "      <td>1.039308</td>\n",
       "      <td>0.045597</td>\n",
       "      <td>117037.735849</td>\n",
       "      <td>125366.110063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276334</td>\n",
       "      <td>45289.168239</td>\n",
       "      <td>117921.644654</td>\n",
       "      <td>19721.0</td>\n",
       "      <td>1.048742</td>\n",
       "      <td>1.048742</td>\n",
       "      <td>1.040881</td>\n",
       "      <td>1.050314</td>\n",
       "      <td>374.852201</td>\n",
       "      <td>1.040881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117888</th>\n",
       "      <th>117885</th>\n",
       "      <td>8102.796526</td>\n",
       "      <td>0.962779</td>\n",
       "      <td>36908.307692</td>\n",
       "      <td>1.034739</td>\n",
       "      <td>115.603400</td>\n",
       "      <td>116779.330025</td>\n",
       "      <td>1.074442</td>\n",
       "      <td>0.069479</td>\n",
       "      <td>116375.397022</td>\n",
       "      <td>118895.178660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287161</td>\n",
       "      <td>47493.826303</td>\n",
       "      <td>117975.982630</td>\n",
       "      <td>117887.0</td>\n",
       "      <td>1.027295</td>\n",
       "      <td>1.052109</td>\n",
       "      <td>1.042184</td>\n",
       "      <td>1.039702</td>\n",
       "      <td>1087.583127</td>\n",
       "      <td>1.062035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117898</th>\n",
       "      <th>117896</th>\n",
       "      <td>7074.697368</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>39311.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.133816</td>\n",
       "      <td>118601.684211</td>\n",
       "      <td>1.039474</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>115509.934211</td>\n",
       "      <td>129160.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444761</td>\n",
       "      <td>50768.394737</td>\n",
       "      <td>118441.986842</td>\n",
       "      <td>117887.0</td>\n",
       "      <td>1.078947</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>226907.223684</td>\n",
       "      <td>1.092105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117900</th>\n",
       "      <th>117899</th>\n",
       "      <td>8412.548387</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>39978.685484</td>\n",
       "      <td>1.024194</td>\n",
       "      <td>14.631048</td>\n",
       "      <td>118152.564516</td>\n",
       "      <td>1.064516</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>112783.467742</td>\n",
       "      <td>132243.411290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594241</td>\n",
       "      <td>36793.814516</td>\n",
       "      <td>117918.943548</td>\n",
       "      <td>19721.0</td>\n",
       "      <td>1.040323</td>\n",
       "      <td>1.016129</td>\n",
       "      <td>1.040323</td>\n",
       "      <td>1.032258</td>\n",
       "      <td>67.983871</td>\n",
       "      <td>1.024194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117908</th>\n",
       "      <th>117905</th>\n",
       "      <td>8248.275824</td>\n",
       "      <td>0.966499</td>\n",
       "      <td>43784.246231</td>\n",
       "      <td>1.048576</td>\n",
       "      <td>7.406131</td>\n",
       "      <td>118656.426019</td>\n",
       "      <td>1.035734</td>\n",
       "      <td>0.056393</td>\n",
       "      <td>117291.645449</td>\n",
       "      <td>144449.128978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232762</td>\n",
       "      <td>20207.386376</td>\n",
       "      <td>119778.709659</td>\n",
       "      <td>290919.0</td>\n",
       "      <td>1.045226</td>\n",
       "      <td>1.039084</td>\n",
       "      <td>1.050251</td>\n",
       "      <td>1.046901</td>\n",
       "      <td>689.810162</td>\n",
       "      <td>1.045226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247660</th>\n",
       "      <th>247659</th>\n",
       "      <td>8640.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46087.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.204000</td>\n",
       "      <td>118955.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118935.400000</td>\n",
       "      <td>166596.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913296</td>\n",
       "      <td>5012.400000</td>\n",
       "      <td>119613.200000</td>\n",
       "      <td>19721.0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254396</th>\n",
       "      <th>235351</th>\n",
       "      <td>7311.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24926.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>117969.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117961.000000</td>\n",
       "      <td>254394.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610709</td>\n",
       "      <td>16573.000000</td>\n",
       "      <td>196823.000000</td>\n",
       "      <td>254395.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258436</th>\n",
       "      <th>258434</th>\n",
       "      <td>6212.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37137.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>118343.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117961.000000</td>\n",
       "      <td>283557.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.526836</td>\n",
       "      <td>4887.000000</td>\n",
       "      <td>120126.000000</td>\n",
       "      <td>118424.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266863</th>\n",
       "      <th>266862</th>\n",
       "      <td>6852.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78844.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>118052.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117961.000000</td>\n",
       "      <td>123688.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316311</td>\n",
       "      <td>111636.000000</td>\n",
       "      <td>119972.000000</td>\n",
       "      <td>123689.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270691</th>\n",
       "      <th>270690</th>\n",
       "      <td>8307.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75077.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>118316.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118315.000000</td>\n",
       "      <td>168175.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966333</td>\n",
       "      <td>92887.000000</td>\n",
       "      <td>140453.000000</td>\n",
       "      <td>19721.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Id         Y            f1        f2          f3  \\\n",
       "f13    f19                                                                 \n",
       "117880 117879  8201.386792  0.886792  37070.622642  1.042453    8.744245   \n",
       "117888 117885  8102.796526  0.962779  36908.307692  1.034739  115.603400   \n",
       "117898 117896  7074.697368  0.986842  39311.000000  1.000000   12.133816   \n",
       "117900 117899  8412.548387  0.919355  39978.685484  1.024194   14.631048   \n",
       "117908 117905  8248.275824  0.966499  43784.246231  1.048576    7.406131   \n",
       "...                    ...       ...           ...       ...         ...   \n",
       "247660 247659  8640.200000  1.000000  46087.600000  1.000000    9.204000   \n",
       "254396 235351  7311.500000  1.000000  24926.500000  1.000000    1.770000   \n",
       "258436 258434  6212.000000  1.000000  37137.000000  1.000000    1.770000   \n",
       "266863 266862  6852.000000  1.000000  78844.000000  2.000000    1.770000   \n",
       "270691 270690  8307.000000  1.000000  75077.000000  1.000000    3.540000   \n",
       "\n",
       "                          f4        f5        f6             f7  \\\n",
       "f13    f19                                                        \n",
       "117880 117879  116838.770440  1.039308  0.045597  117037.735849   \n",
       "117888 117885  116779.330025  1.074442  0.069479  116375.397022   \n",
       "117898 117896  118601.684211  1.039474  0.013158  115509.934211   \n",
       "117900 117899  118152.564516  1.064516  0.032258  112783.467742   \n",
       "117908 117905  118656.426019  1.035734  0.056393  117291.645449   \n",
       "...                      ...       ...       ...            ...   \n",
       "247660 247659  118955.000000  1.000000  0.000000  118935.400000   \n",
       "254396 235351  117969.000000  1.000000  0.000000  117961.000000   \n",
       "258436 258434  118343.000000  1.000000  0.000000  117961.000000   \n",
       "266863 266862  118052.000000  1.000000  0.000000  117961.000000   \n",
       "270691 270690  118316.000000  1.000000  0.000000  118315.000000   \n",
       "\n",
       "                          f8  ...       f14            f15            f16  \\\n",
       "f13    f19                    ...                                           \n",
       "117880 117879  125366.110063  ...  0.276334   45289.168239  117921.644654   \n",
       "117888 117885  118895.178660  ...  0.287161   47493.826303  117975.982630   \n",
       "117898 117896  129160.250000  ...  0.444761   50768.394737  118441.986842   \n",
       "117900 117899  132243.411290  ...  0.594241   36793.814516  117918.943548   \n",
       "117908 117905  144449.128978  ...  0.232762   20207.386376  119778.709659   \n",
       "...                      ...  ...       ...            ...            ...   \n",
       "247660 247659  166596.600000  ... -0.913296    5012.400000  119613.200000   \n",
       "254396 235351  254394.000000  ...  0.610709   16573.000000  196823.000000   \n",
       "258436 258434  283557.000000  ... -4.526836    4887.000000  120126.000000   \n",
       "266863 266862  123688.000000  ... -0.316311  111636.000000  119972.000000   \n",
       "270691 270690  168175.000000  ... -0.966333   92887.000000  140453.000000   \n",
       "\n",
       "                    f17       f18       f20       f21       f22  \\\n",
       "f13    f19                                                        \n",
       "117880 117879   19721.0  1.048742  1.048742  1.040881  1.050314   \n",
       "117888 117885  117887.0  1.027295  1.052109  1.042184  1.039702   \n",
       "117898 117896  117887.0  1.078947  1.052632  1.105263  1.052632   \n",
       "117900 117899   19721.0  1.040323  1.016129  1.040323  1.032258   \n",
       "117908 117905  290919.0  1.045226  1.039084  1.050251  1.046901   \n",
       "...                 ...       ...       ...       ...       ...   \n",
       "247660 247659   19721.0  1.200000  1.000000  1.000000  1.000000   \n",
       "254396 235351  254395.0  1.000000  1.000000  1.000000  1.000000   \n",
       "258436 258434  118424.0  1.000000  1.000000  1.000000  1.000000   \n",
       "266863 266862  123689.0  1.000000  1.000000  1.000000  1.000000   \n",
       "270691 270690   19721.0  1.000000  1.000000  1.000000  1.000000   \n",
       "\n",
       "                         f23       f24  \n",
       "f13    f19                              \n",
       "117880 117879     374.852201  1.040881  \n",
       "117888 117885    1087.583127  1.062035  \n",
       "117898 117896  226907.223684  1.092105  \n",
       "117900 117899      67.983871  1.024194  \n",
       "117908 117905     689.810162  1.045226  \n",
       "...                      ...       ...  \n",
       "247660 247659       5.200000  1.200000  \n",
       "254396 235351       6.000000  1.000000  \n",
       "258436 258434       1.000000  1.000000  \n",
       "266863 266862       1.000000  1.000000  \n",
       "270691 270690       2.000000  1.000000  \n",
       "\n",
       "[322 rows x 24 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['f13', 'f19']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f13 and f19 represent the same data. One of the two features can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (16383, 25)\n",
      "test shape: (16385, 24)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicated column\n",
    "train.drop('f13', axis=1, inplace=True)\n",
    "test.drop('f13', axis=1, inplace=True)\n",
    "print('train shape: ', train.shape)\n",
    "print('test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id     16383\n",
       "Y          2\n",
       "f1      5170\n",
       "f2         7\n",
       "f3       168\n",
       "f4       162\n",
       "f5         8\n",
       "f6         9\n",
       "f7       118\n",
       "f8      1851\n",
       "f9         7\n",
       "f10      182\n",
       "f11        7\n",
       "f12      157\n",
       "f14    11349\n",
       "f15     3555\n",
       "f16      432\n",
       "f17       67\n",
       "f18        9\n",
       "f19      322\n",
       "f20        7\n",
       "f21        8\n",
       "f22        7\n",
       "f23      906\n",
       "f24        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()\n",
    "#trainu = train.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing data\n",
    "No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import missingno as msno \n",
    "#msno.matrix(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['f14'].hist(bins = 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'f1':'f24'],\n",
    "                      test.loc[:,'f1':'f24']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25884</td>\n",
       "      <td>1</td>\n",
       "      <td>33.63</td>\n",
       "      <td>118596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118595</td>\n",
       "      <td>125738</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1945</td>\n",
       "      <td>118450</td>\n",
       "      <td>119184</td>\n",
       "      <td>1</td>\n",
       "      <td>121372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34346</td>\n",
       "      <td>1</td>\n",
       "      <td>10.62</td>\n",
       "      <td>118041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117902</td>\n",
       "      <td>130913</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>15385</td>\n",
       "      <td>117945</td>\n",
       "      <td>292795</td>\n",
       "      <td>1</td>\n",
       "      <td>259173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34923</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>118327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>124402</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7547</td>\n",
       "      <td>118933</td>\n",
       "      <td>290919</td>\n",
       "      <td>1</td>\n",
       "      <td>118784</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80926</td>\n",
       "      <td>1</td>\n",
       "      <td>30.09</td>\n",
       "      <td>118300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117961</td>\n",
       "      <td>301218</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4933</td>\n",
       "      <td>118458</td>\n",
       "      <td>118331</td>\n",
       "      <td>1</td>\n",
       "      <td>307024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4674</td>\n",
       "      <td>1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>119921</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119920</td>\n",
       "      <td>302830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13836</td>\n",
       "      <td>142145</td>\n",
       "      <td>4673</td>\n",
       "      <td>1</td>\n",
       "      <td>128230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1  f2     f3      f4  f5  f6      f7      f8  f9  f10  ...    f15  \\\n",
       "0  25884   1  33.63  118596   1   0  118595  125738   1    3  ...   1945   \n",
       "1  34346   1  10.62  118041   1   0  117902  130913   1    1  ...  15385   \n",
       "2  34923   1   1.77  118327   1   0  117961  124402   1    2  ...   7547   \n",
       "3  80926   1  30.09  118300   1   0  117961  301218   1    0  ...   4933   \n",
       "4   4674   1   1.77  119921   1   0  119920  302830   1    0  ...  13836   \n",
       "\n",
       "      f16     f17  f18     f19  f20  f21  f22  f23  f24  \n",
       "0  118450  119184    1  121372    1    1    1    2    1  \n",
       "1  117945  292795    1  259173    1    1    1    1    1  \n",
       "2  118933  290919    1  118784    1    1    1    1    1  \n",
       "3  118458  118331    1  307024    1    1    1    2    1  \n",
       "4  142145    4673    1  128230    1    1    1  620    1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32768, 23)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Features shape: (32768, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   -2.266430\n",
       "1   -0.305612\n",
       "2    2.015561\n",
       "3   -3.172501\n",
       "4    0.573767\n",
       "Name: f14, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('Train shape:', train.shape)\n",
    "#print('Test shape:', test.shape)\n",
    "\n",
    "train_labels = train['Y'].reset_index(drop=True)\n",
    "train_features = train.drop(['Y'], axis=1)\n",
    "test_features = test\n",
    "\n",
    "# All features are combined here.\n",
    "# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\n",
    "all_features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "print('All_Features shape:', all_features.shape)\n",
    "all_features['f14'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features are f2, f5, f6, f9, f11, f18, f20, f21, f22, f24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24]\n",
      "['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24']\n"
     ]
    }
   ],
   "source": [
    "# Finding numeric features\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "cat_features = np.where(train.dtypes != np.float)[0]\n",
    "print(cat_features)\n",
    "all_cols = train.columns\n",
    "all_cols = all_cols.drop('Y')\n",
    "all_cols = all_cols.drop('Id')\n",
    "cat_cols = all_cols.drop('f14')\n",
    "print(list(cat_cols))\n",
    "\n",
    "\n",
    "numeric = []\n",
    "categorical = []\n",
    "for i in train.columns:\n",
    "    if train[i].dtype in cat_cols:\n",
    "        if i in cat_features.tolist():\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numeric.append(i)\n",
    "\n",
    "for i, feature in enumerate(list(train[categorical]), 1):\n",
    "    print(i, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding features shape: (32768, 24)\n",
      "After encoding features shape: (32768, 17512)\n",
      "After removing duplicates features shape: (32768, 17512)\n",
      "train shape b4:  (16383, 25)\n",
      "test shape b4: (16385, 24)\n",
      "X_Train shape:  (16383, 17512)\n",
      "X_Test shape: (16385, 17512)\n",
      "y shape: (16383,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "all_data = pd.get_dummies(all_features)\n",
    "print('Before encoding features shape:', all_features.shape)\n",
    "# get_dummies converts categorical to numerical data and does one-hot encoding\n",
    "# Categorical features are f2, f5, f6, f9, f11, f18, f20, f21, f22, f24\n",
    "# binary encode\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, drop='first')\n",
    "'''\n",
    "#print(encdf_f2.head())\n",
    "encdf_f2 = pd.DataFrame(enc.fit_transform(all_data[['f2']]))\n",
    "encdf_f5 = pd.DataFrame(enc.fit_transform(all_data[['f5']]))\n",
    "encdf_f6 = pd.DataFrame(enc.fit_transform(all_data[['f6']]))\n",
    "encdf_f9 = pd.DataFrame(enc.fit_transform(all_data[['f9']]))\n",
    "encdf_f11 = pd.DataFrame(enc.fit_transform(all_data[['f11']]))\n",
    "encdf_f18 = pd.DataFrame(enc.fit_transform(all_data[['f18']]))\n",
    "encdf_f20 = pd.DataFrame(enc.fit_transform(all_data[['f20']]))\n",
    "encdf_f21 = pd.DataFrame(enc.fit_transform(all_data[['f21']]))\n",
    "encdf_f22 = pd.DataFrame(enc.fit_transform(all_data[['f22']]))\n",
    "encdf_f24 = pd.DataFrame(enc.fit_transform(all_data[['f24']]))\n",
    "#print(encdf_f2.head())\n",
    "#print(encdf_f5.head())\n",
    "dfs = [encdf_f2, encdf_f5, encdf_f6, encdf_f9, encdf_f11, encdf_f18, encdf_f20, encdf_f21, encdf_f22 ,encdf_f24]\n",
    "enc_dfs = pd.concat(dfs, ignore_index = True, axis=1)\n",
    "print(enc_dfs.shape)\n",
    "#print(enc_dfs.head())\n",
    "\n",
    "all_data = all_data.join([enc_dfs])\n",
    "'''\n",
    "\n",
    "#print(all_data.head())\n",
    "\n",
    "all_data = pd.DataFrame(enc.fit_transform(all_data[cat_cols]))\n",
    "print('After encoding features shape:',all_data.shape)\n",
    "\n",
    "# Remove any duplicated column names\n",
    "all_data = all_data.loc[:,~all_data.columns.duplicated()]\n",
    "print('After removing duplicates features shape:',all_data.shape)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "#all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "print('train shape b4: ', train.shape)\n",
    "print('test shape b4:', test.shape)\n",
    "#creating matrices for sklearn:\n",
    "X_Train = all_data[:train.shape[0]]\n",
    "X_Test = all_data[train.shape[0]:]\n",
    "y = train.Y\n",
    "\n",
    "print('X_Train shape: ', X_Train.shape)\n",
    "print('X_Test shape:', X_Test.shape)\n",
    "print('y shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_value_stats = X_Train.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]\n",
    "\n",
    "null_value_stats = X_Test.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "X_Train.fillna(-999, inplace=True)\n",
    "X_Test.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17502</th>\n",
       "      <th>17503</th>\n",
       "      <th>17504</th>\n",
       "      <th>17505</th>\n",
       "      <th>17506</th>\n",
       "      <th>17507</th>\n",
       "      <th>17508</th>\n",
       "      <th>17509</th>\n",
       "      <th>17510</th>\n",
       "      <th>17511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 17512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9      \\\n",
       "16383    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16384    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16385    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16386    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16387    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...  17502  17503  17504  17505  17506  17507  17508  17509  17510  \\\n",
       "16383  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16384  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16385  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16386  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "16387  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       17511  \n",
       "16383    0.0  \n",
       "16384    0.0  \n",
       "16385    0.0  \n",
       "16386    0.0  \n",
       "16387    0.0  \n",
       "\n",
       "[5 rows x 17512 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17502</th>\n",
       "      <th>17503</th>\n",
       "      <th>17504</th>\n",
       "      <th>17505</th>\n",
       "      <th>17506</th>\n",
       "      <th>17507</th>\n",
       "      <th>17508</th>\n",
       "      <th>17509</th>\n",
       "      <th>17510</th>\n",
       "      <th>17511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 17512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   17502  17503  17504  17505  17506  17507  17508  17509  17510  17511  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 17512 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.head()\n",
    "#X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Data preprocessing\n",
    "\n",
    "# Playing with CVs\n",
    "Play with CVs to get different n-fold partitions and see if that helps improve the rmse mean that I receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, lasso_path\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, lasso_path, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "folds = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = RANDOM_SEED)\n",
    "\n",
    "\n",
    "# Function to return the rmse loss\n",
    "# cv = 5 means 5 folds, check if it works better with more/less number of folds\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_Train, y, scoring=\"neg_mean_squared_error\", cv = 10))\n",
    "    return(rmse)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X=X_Train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "def cv_auc(model, X=X_Train):\n",
    "    score_auc = -cross_val_score(model, X, y, scoring=\"roc_auc\", cv=kfolds, n_jobs = 20)\n",
    "    return (score_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Hyperopt to optimize over hyperparameters for each model\n",
    "\n",
    "Hyperparameters over which tuning was performed:\n",
    "1. Classifier penalty\n",
    "2. Number of C values to search over\n",
    "3. Classifier type\n",
    "4. Classifier solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_Train, y, train_size=0.8, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert NumPy data to DMatrix format\n",
    "With out data simulated and formatted as NumPy arrays, our next step is to convert this to a DMatrix object that XGBoost can work with. We can instantiate an object of the xgboost.DMatrix by passing in the feature matrix as the first argument followed by the label vector using the label= keyword argument. To learn more about XGBoost's support for data structures other than NumPy arrays, see the documentation for the Data Interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.22 s, sys: 220 ms, total: 9.44 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalidation = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set Parameters\n",
    "There are a number of parameters that can be set before XGBoost can be run.\n",
    "\n",
    "* General parameters relate to which booster we are using to do boosting, commonly tree or linear model\n",
    "* Booster parameters depend on which booster you have chosen\n",
    "* Learning task parameters decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\n",
    "For more information on the configurable parameters within the XGBoost module, see the documentation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': 1, 'tree_method': 'gpu_hist', 'n_gpus': 1, 'eval_metric': 'auc', 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = 1\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus\n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {'eval_metric': 'auc', 'objective': 'binary:logistic'}\n",
    "params.update(learning_task_params)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train Model\n",
    "Now it's time to train our model! We can use the xgb.train function and pass in the parameters, training dataset, the number of boosting iterations, and the list of items to be evaluated during training. For more information on the parameters that can be passed into xgb.train, check out the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model training settings\n",
    "evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "num_round = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:27:37] WARNING: /opt/conda/envs/rapids/conda-bld/xgboost_1603299470061/work/include/xgboost/generic_parameters.h:35: \n",
      "n_gpus: \n",
      "\tDeprecated. Single process multi-GPU training is no longer supported.\n",
      "\tPlease switch to distributed training with one process per GPU.\n",
      "\tThis can be done using Dask or Spark.  See documentation for details.\n",
      "[17:27:37] WARNING: /opt/conda/envs/rapids/conda-bld/xgboost_1603299470061/work/src/learner.cc:529: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-auc:0.53929\ttrain-auc:0.56502\n",
      "[1]\tvalidation-auc:0.54717\ttrain-auc:0.60114\n",
      "[2]\tvalidation-auc:0.55196\ttrain-auc:0.61410\n",
      "[3]\tvalidation-auc:0.55441\ttrain-auc:0.62367\n",
      "[4]\tvalidation-auc:0.55909\ttrain-auc:0.63099\n",
      "[5]\tvalidation-auc:0.58487\ttrain-auc:0.66160\n",
      "[6]\tvalidation-auc:0.58492\ttrain-auc:0.66186\n",
      "[7]\tvalidation-auc:0.61218\ttrain-auc:0.70012\n",
      "[8]\tvalidation-auc:0.64486\ttrain-auc:0.74079\n",
      "[9]\tvalidation-auc:0.65753\ttrain-auc:0.76104\n",
      "CPU times: user 4.85 s, sys: 848 ms, total: 5.7 s\n",
      "Wall time: 4.15 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-810bc0914f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# timing starts from this point for \"start_time\" variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m best_xgbm = fmin(fn=hyperparameter_tuning_xgbm,\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_trials_to_calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m     \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDomain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     rval = FMinIter(\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, expr, workdir, pass_expr_memo_ctrl, name, loss_target)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# -- raises exception if expr contains cycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0mpyll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizeHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_new_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# -- raises exception if v_expr contains cycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/pyll/base.py\u001b[0m in \u001b[0;36mtoposort\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_in\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopological_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "#from catboost import CatBoostClassifier, cv, Pool\n",
    "\n",
    "#COmmented out as it takes too long to run. \n",
    "#Under construction, some things can be improved.\n",
    "space = {\n",
    "       hp.uniform('max_depth', 5, 12),\n",
    "       hp.uniform('max_features', 0.5, 1.0),\n",
    "       hp.quniform('n_estimators', 1000 , 5000, 500),\n",
    "    }\n",
    "\n",
    "\n",
    "def hyperparameter_tuning_xgbm(space):\n",
    "    xgboost1 = XGBClassifier(\n",
    "                       n_estimators=space['n_estimators'],\n",
    "                       booster='gbtree',\n",
    "                       max_depth=space['max_depth'],\n",
    "                       max_features=space['max_features'],\n",
    "                       gamma=0.6,\n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='logloss',\n",
    "                       nthread=1,\n",
    "                       n_jobs=16,\n",
    "                       #scale_pos_weight=1,\n",
    "                       silent=0,\n",
    "                       seed=42,\n",
    "                       reg_alpha=0.0006,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       #predictor = 'gpu_predictor',\n",
    "                       #gpu_id=0,\n",
    "                       random_state=1001)\n",
    "\n",
    "    #model.fit(X_train, y_train, cat_features=cat_features,use_best_model=True,\n",
    "    #          verbose=False, eval_set=(X_val, y_val))\n",
    "    #preds_class = model.predict_proba(X_val)\n",
    "    #score = classification_report(y_val, preds_class, output_dict=True)['0']['f1-score']\n",
    "    \n",
    "    score = cross_val_score(xgboost1, X_Train, y, scoring='roc_auc', cv=skf).mean()\n",
    "    print(\"AUC {:.3f} params {}\".format(score, space))\n",
    "    return score\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "\n",
    "best_xgbm = fmin(fn=hyperparameter_tuning_xgbm,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "\n",
    "print(best_xgbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/35 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Unable to allocate 438. MiB for an array with shape (3277, 17512) and data type float64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:52<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 438. MiB for an array with shape (3277, 17512) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-e934d6337937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m best = fmin(fn=objective,\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             )\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-e934d6337937>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gini {:.3f} params {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 242\u001b[0;31m     scores = parallel(\n\u001b[0m\u001b[1;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/local/lambda/rishabhs/anaconda3/envs/rapids-0.16/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 438. MiB for an array with shape (3277, 17512) and data type float64"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        tree_method= 'gpu_hist',\n",
    "        n_gpus = 1,\n",
    "        booster='gbtree',\n",
    "        nthread=1,\n",
    "        n_jobs=10,\n",
    "        eval_metric='auc',\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X_Train, y, scoring='roc_auc', n_jobs = 10, cv=StratifiedKFold()).mean()\n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 12, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best XGBClassifier model1 \n",
    "AUC 0.777 params {'colsample_bytree': 0.6234941789494903, 'num_leaves': 104.0} \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
